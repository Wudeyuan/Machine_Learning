# Machine_Learning
机器学习
1. **什么是机器学习？**  
- 定义有所不同：1）根据经验不断优化。2）把无序的数据转换成有用的信息。  
- 主要任务：分类（监督）、回归（监督）、聚类（无监督）
- 涉及神经网络的机器学习叫深度学习
2. **学习的种类**  
监督与无监督：监督学习指的是“利用经验获取技能”，然后利用“技能”来“监督”测试集。无监督学习的训练集和测试集没有区别，聚类是典型的例子。还有一种中间情况，监督并不明确（如俄罗斯方块），称为强化学习。
3. **分类算法**
- **k邻近算法（KNN）：**  
优点：精度高、对异常值不敏感、无数据输入假定。  
缺点：计算复杂度高、空间复杂度高。  
适用数据范围：数值型和标称型。   
对未知类别属性的数据集中的每个点依次执行以下操作：  
(1) 计算已知类别数据集中的点与当前点之间的距离；  
(2) 按照距离（一般欧氏距离）递增次序排序；  
(3) 选取与当前点距离最小的k个点；  
(4) 确定前k个点所在类别的出现频率；  
(5) 返回前k个点出现频率最高的类别作为当前点的预测分类。
- **决策树：**  
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。  
缺点：可能会产生过度匹配问题。  
适用数据类型：数值型和标称型。  
决策树的一般流程：  
(1) 收集数据：可以使用任何方法。  
(2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。  
(3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。  
(4) 训练算法：构造树的数据结构。  
(5) 测试算法：使用经验树计算错误率。  
(6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。
- **朴素贝叶斯**  
优点：在数据较少的情况下仍然有效，可以处理多类别问题。  
缺点：对于输入数据的准备方式较为敏感。  
适用数据类型：标称型数据。  
朴素贝叶斯的一般过程：  
(1) 收集数据：可以使用任何方法。本章使用RSS源。  
(2) 准备数据：需要数值型或者布尔型数据。  
(3) 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。  
(4) 训练算法：计算不同的独立特征的条件概率。  
(5) 测试算法：计算错误率。  
(6) 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。  
- **Logistic回归**   
优点：计算代价不高，易于理解和实现。  
缺点：容易欠拟合，分类精度可能不高。  
适用数据类型：数值型和标称型数据。  
一般过程  
(1) 收集数据：采用任意方法收集数据。  
(2) 准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据
格式则最佳。  
(3) 分析数据：采用任意方法对数据进行分析。  
(4) 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。  
(5) 测试算法：一旦训练步骤完成，分类将会很快。  
(6) 使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。
- **支持向量机**  
优点：泛化错误率低，计算开销不大，结果易解释。  
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。  
适用数据类型：数值型和标称型数据。  
一般框架：  
(1) 收集数据：可以使用任意方法。  
(2) 准备数据：需要数值型数据。  
(3) 分析数据：有助于可视化分隔超平面。  
(4) 训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优。  
(5) 测试算法：十分简单的计算过程就可以实现。  
(6) 使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。  
- **AdaBoost元算法（集成算法）**  
优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整。  
缺点：对离群点敏感。  
适用数据类型：数值型和标称型数据。  
AdaBoost的一般流程：  
(1) 收集数据：可以使用任意方法。  
(2) 准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器。作为弱分类器，简单分类器的效果更好。  
(3) 分析数据：可以使用任意方法。  
(4) 训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。  
(5) 测试算法：计算分类的错误率。  
(6) 使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类别的场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改。  
4. **数值算法**
- **回归：**   
优点：结果易于理解，计算上不复杂。  
缺点：对非线性的数据拟合不好。  
适用数据类型：数值型和标称型数据。  
(1) 收集数据：采用任意方法收集数据。  
(2) 准备数据：回归需要数值型数据，标称型数据将被转成二值型数据。  
(3) 分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法  求得新回归系数之后，可以将新拟合线绘在图上作为对比。  
(4) 训练算法：找到回归系数。  
(5) 测试算法：使用R2或者预测值和数据的拟合度，来分析模型的效果。  
(6) 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签  
- **树回归：**  
优点：可以对复杂和非线性的数据建模。  
缺点：结果不易理解。  
适用数据类型：数值型和标称型数据。  
树回归的一般方法  
(1) 收集数据：采用任意方法收集数据。  
(2) 准备数据：需要数值型的数据，标称型数据应该映射成二值型数据。  
(3) 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。  
(4) 训练算法：大部分时间都花费在叶节点树模型的构建上。  
(5) 测试算法：使用测试数据上的R2值来分析模型的效果。  
(6) 使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情  
5. **无监督学习**
- **K-means**  
优点：容易实现。  
缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢。  
适用数据类型：数值型数据。  
K-均值聚类的一般流程  
(1) 收集数据：使用任意方法。  
(2) 准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算。  
(3) 分析数据：使用任意方法。   
(4) 训练算法：不适用于无监督学习，即无监督学习没有训练过程。  
(5) 测试算法：应用聚类算法、观察结果。可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果。  
(6) 使用算法：可以用于所希望的任何应用。通常情况下，簇质心可以代表整个簇的数据来做出决策。  
